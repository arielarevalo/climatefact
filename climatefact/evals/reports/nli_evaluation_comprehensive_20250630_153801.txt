
COMPREHENSIVE NLI MODEL EVALUATION REPORT
=========================================
Generated: 2025-06-30T15:38:01.189062
Evaluator: Full NLI Evaluation Runner

DATASET ANALYSIS
===============
Total entries in gold set: 846
Entries with entailment labels: 726
Entries without entailment labels: 120
Coverage: 85.8% of entries have entailment labels

Entailment Label Distribution in Gold Set:
{
  "ENTAILMENT": 241,
  "CONTRADICTION": 241,
  "NEUTRAL": 244
}


NLI EVALUATION SUMMARY
=====================
Total Predictions: 400
Overall Accuracy: 0.6150

MACRO AVERAGES
--------------
Precision: 0.6035
Recall: 0.6083
F1-Score: 0.5990

PER-CLASS METRICS
-----------------
ENTAILMENT:
  Precision: 0.5243
  Recall: 0.6750
  F1-Score: 0.5902
CONTRADICTION:
  Precision: 0.9914
  Recall: 0.9583
  F1-Score: 0.9746
NEUTRAL:
  Precision: 0.2949
  Recall: 0.1917
  F1-Score: 0.2323

CONFUSION MATRIX
----------------
True\Predicted	ENT	CON	NEU
ENT		108	1	51
CON		1	115	4
NEU		97	0	23

RECOMMENDATIONS
==============

- Model accuracy is moderate (60-80%). There's room for improvement.
- NEUTRAL class performs poorly (F1=0.232). Consider class-specific improvements.

DATA FILES USED
==============
Gold Set: /Users/ariel/workspace/ci0129/proyecto/data/evaluation/gold_set.jsonl
Passages: /Users/ariel/workspace/ci0129/proyecto/data/passages.jsonl

This evaluation tests the NLI model's ability to classify entailment relationships
between climate-related claims and evidence passages.
