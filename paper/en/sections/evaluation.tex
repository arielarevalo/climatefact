\section{Evaluation}

The evaluation of the ClimateFact system is structured around a reproducible methodology that measures the effectiveness of each component in the verification workflow. The evaluation process is based on the construction of a reference set (\textit{gold set}) and the application of standard metrics for information retrieval and text classification systems.

\subsection{Reference set construction}

A reference set composed of 400 claim-evidence pairs extracted from the IPCC scientific corpus was constructed. Each entry in the set includes a climate change claim and the identifier of the passage that constitutes the most relevant evidence within the knowledge base. The selection of these pairs was carried out through a semi-automatic process that ensures semantic correspondence between claims and evidence.

Additionally, control cases representing approximately 10\% of the total set were incorporated. These cases include: (i) claims with no corresponding evidence in the corpus, (ii) claims accompanied by distractor passages that do not provide relevant evidence, and (iii) deliberately ambiguous claims that allow evaluating the system's robustness against imprecise queries.

For the specific evaluation of the logical inference component, each claim-evidence pair was manually labeled according to the logical relationship established by the passage with respect to the claim: \textit{entailment} (the evidence implies the claim), \textit{contradiction} (the evidence contradicts the claim), or \textit{neutral} (the evidence neither confirms nor refutes the claim).

\subsection{Retrieval evaluation metrics}

The quality of the retrieval subsystem is evaluated using standard information retrieval metrics, applied for different values of $k$ (number of retrieved documents):

\begin{itemize}
\item \textbf{Recall@k}: Fraction of relevant evidence retrieved among the top $k$ results.
\item \textbf{Precision@k}: Fraction of relevant documents among the $k$ retrieved documents.
\item \textbf{MRR@k}: Mean Reciprocal Rank, which measures the position of the first relevant document in the ranking.
\item \textbf{nDCG@k}: Normalized Discounted Cumulative Gain, which weights relevance by position in the ranking.
\end{itemize}

These metrics are calculated independently for each retrieval strategy (conceptual hybrid, semantic embedding, and fused combination) with values of $k \in \{1, 3, 5, 10\}$, allowing evaluation of both the precision and exhaustiveness of the retrieval process.

\subsection{Logical inference evaluation metrics}

The effectiveness of the natural language inference model is evaluated using multiclass classification metrics:

\begin{itemize}
\item \textbf{Overall accuracy}: Fraction of correct predictions over the total number of evaluated cases.
\item \textbf{Per-class Precision, Recall, and F1-score}: Calculated independently for each logical relationship label.
\item \textbf{Macro and micro averages}: Aggregation of individual metrics to obtain a global classifier evaluation.
\item \textbf{Confusion matrix}: Detailed analysis of classification errors between categories.
\end{itemize}

\subsection{Evaluation protocol}

The evaluation protocol ensures reproducibility through strict separation between training and evaluation data. The reference set is maintained independently of any system tuning or calibration process, ensuring an objective evaluation.

The evaluation is executed automatically through scripts that process the complete set of test cases, record system predictions, and calculate all specified metrics. Results are stored in a structured format that facilitates both statistical analysis and the generation of detailed reports.

This methodological approach allows identifying specific strengths and weaknesses in each system component, providing valuable information for future development iterations.
