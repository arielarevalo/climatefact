\section{Related Work}

In recent years, artificial intelligence has established itself as a key area in technological development, leading to a significant increase in research linked to its multiple applications. This surge is explained by AI's ability to offer innovative and effective responses to complex problems across various sectors, including education, business, healthcare, and the environment. From this perspective, the present state of the art explores the main scientific advances in the use of artificial intelligence for tasks such as natural language processing, automatic data verification, and the creation of large-scale language models.

Chavarr\'{i}a Mu\~{n}oz~\cite{chavarria2022} developed a web application prototype for fake news detection in Spanish, integrating classification algorithms such as \textit{Support Vector Machine}, \textit{Random Forest}, and \textit{Passive Aggressive Classifier}, along with natural language processing techniques including sentiment analysis, topic modeling, and profanity detection. The study used a balanced corpus of 1,248 news articles (half fake and half real), achieving accuracy levels close to 72\% with the \textit{Random Forest} classifier. Through the use of tools such as Python, Streamlit, and Heroku, the prototype allowed users to enter a news headline and body, receiving an estimated veracity percentage along with associated linguistic metrics, representing a relevant contribution to the automation of critical analysis of digital content in Spanish~\cite{chavarria2022}.

Rey Morales~\cite{rey2025fakenews} developed a model for fake news detection combining various natural language processing techniques and classification algorithms. The study implemented approaches such as TF-IDF, Bag of Words, Word2Vec, and the DistilBERT transformer model, along with classifiers including logistic regression, Support Vector Machines, and Random Forest. Additionally, techniques such as dimensionality reduction with PCA and cross-validation were applied to improve accuracy. One of the main conclusions was that, while advanced models like BERT achieved high accuracy levels (above 99\%), contextual analysis of elements external to the news article---such as comments, source, and similarity to other articles---is essential for achieving more reliable predictions. This work contributes to the state of the art by offering an exhaustive comparison of methods and highlighting the importance of context in automated disinformation detection.

Mafla Checa~\cite{mafla2021noticias}, in a work titled \textit{``Automatic identification of fake news in Spanish using data mining and natural language processing techniques''}, proposes a comprehensive methodology spanning from the construction of a corpus of Ecuadorian news articles to the implementation and comparison of supervised classification models. The author manually collected a set of real and fake news articles from national media outlets and political pages, and subsequently applied \textit{natural language processing (NLP)} techniques such as lemmatization, \textit{stop words} removal, and vectorial representation through \textit{TF-IDF}. To address class imbalance, the \textit{SMOTE} oversampling technique was employed. The study evaluates the performance of different classification models, including \textit{Support Vector Machines (SVM)}, \textit{Naive Bayes}, \textit{Random Forest}, and \textit{Boosting}, using metrics such as precision, \textit{F1-score}, and \textit{ROC} curves. Results showed that the most efficient models were SVM and Boosting, with considerably superior performance when exhaustive text preprocessing was applied.
